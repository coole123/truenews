{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News Scrapping",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TAVu6FlsDF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Google News Scraping\n",
        "# !pip install goose3\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "from urllib.parse import urljoin\n",
        "from goose3 import Goose\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.dammit import EncodingDetector\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "#Enter the URL to check for news\n",
        "url = 'https://news.google.com/topics/CAAqIQgKIhtDQkFTRGdvSUwyMHZNRE55YXpBU0FtVnVLQUFQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen'\n",
        "#Append this url for all the articles\n",
        "url1 = 'https://news.google.com'\n",
        "#Match the pattern in all the url's\n",
        "pattern = 'https://news.google.com/articles'\n",
        "#Get Response of url\n",
        "response = requests.get(url)\n",
        "http_encoding = response.encoding if 'charset' in response.headers.get('content-type', '').lower() else None\n",
        "html_encoding = EncodingDetector.find_declared_encoding(response.content, is_html=True)\n",
        "encoding = html_encoding or http_encoding\n",
        "soup = BeautifulSoup(response.content, from_encoding=encoding)\n",
        "\n",
        "final_urls = []\n",
        "#Get the list of hyperlink on the page\n",
        "for link in soup.find_all('a'):\n",
        "  href = link.attrs.get(\"href\")\n",
        "  href = urljoin(url1, href)\n",
        "  if pattern in href:\n",
        "    # final_urls = [href]\n",
        "    final_urls.append(href)\n",
        "\n",
        "#Remove duplicate URL's        \n",
        "unique_urls = set(final_urls)\n",
        "#Convert it back to lists\n",
        "unique_urls_list = list(unique_urls)\n",
        "\n",
        "#Get the Title, Author and Text of Each URL\n",
        "#Create the lists of all variables\n",
        "final_title_list = []\n",
        "final_text_list = []\n",
        "final_source_list = []\n",
        "for data in unique_urls_list:\n",
        "  request = requests.get(data)\n",
        "  g = Goose()\n",
        "  article = g.extract(url=request.url)\n",
        "  title = article.title\n",
        "  text = article.cleaned_text\n",
        "  domain = article.domain\n",
        "  source=re.findall(r'(?<=\\.)([^.]+)(?:\\.(?:co\\.uk|ac\\.us|[^.]+(?:$|\\n)))',domain)\n",
        "  final_title_list.append(title)\n",
        "  final_text_list.append(text)\n",
        "  final_source_list.extend(source)\n",
        "  dict_1 = {'title': final_title_list, 'text': final_text_list, 'author':final_source_list}\n",
        "  df = pd.DataFrame({key: pd.Series(value) for key, value in dict_1.items()})\n",
        "  df.to_csv('test.csv', encoding='utf-8', index=False)\n",
        "#Download the file in your local\n",
        "# files.download(\"test.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90uakRTmPePq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d2ae7ccb-09da-4b14-c0b9-858949aaad5c"
      },
      "source": [
        "#The Hindu Scraping\n",
        "# !pip install goose3\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "from urllib.parse import urljoin\n",
        "from goose3 import Goose\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.dammit import EncodingDetector\n",
        "from google.colab import files\n",
        "\n",
        "url = 'https://www.thehindu.com/'\n",
        "\n",
        "response = requests.get(url)\n",
        "http_encoding = response.encoding if 'charset' in response.headers.get('content-type', '').lower() else None\n",
        "html_encoding = EncodingDetector.find_declared_encoding(response.content, is_html=True)\n",
        "encoding = html_encoding or http_encoding\n",
        "soup = BeautifulSoup(response.content, from_encoding=encoding)\n",
        "\n",
        "final_urls = []\n",
        "for link in soup.select(\"a[href$='.ece']\"):\n",
        "  href = link.attrs.get(\"href\")\n",
        "  final_urls.append(href)\n",
        "#   href = urljoin(url, href)\n",
        "#   if pattern in href:\n",
        "#     # final_urls = [href]\n",
        "        \n",
        "unique_urls = set(final_urls)\n",
        "unique_urls_list = list(unique_urls)\n",
        "\n",
        "\n",
        "#Get the Title, Author and Text of Each URL\n",
        "#Create the lists of all variables\n",
        "final_title_list = []\n",
        "final_text_list = []\n",
        "final_source_list = []\n",
        "for data in unique_urls_list:\n",
        "  # request = requests.get(data)\n",
        "  g = Goose()\n",
        "  article = g.extract(url=data)\n",
        "  title = article.title\n",
        "  text = article.cleaned_text\n",
        "  domain = article.domain\n",
        "  source=re.findall(r'(?<=\\.)([^.]+)(?:\\.(?:co\\.uk|ac\\.us|[^.]+(?:$|\\n)))',domain)\n",
        "  final_title_list.append(title)\n",
        "  final_text_list.append(text)\n",
        "  final_source_list.extend(source)\n",
        "  dict_1 = {'title': final_title_list, 'text': final_text_list, 'author':final_source_list}\n",
        "  df = pd.DataFrame({key: pd.Series(value) for key, value in dict_1.items()})\n",
        "  df.to_csv('TheHindu_DataSet.csv', encoding='utf-8', index=False)\n",
        "# #Download the file in your local\n",
        "files.download(\"TheHindu_DataSet.csv\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_69ad77a2-0ab5-4046-9542-ddc380cc99fc\", \"TheHindu_DataSet.csv\", 526557)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNf1RZD1cZfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fake News \n",
        "#OPTimess scraping\n",
        "# !pip install goose3\n",
        "import requests\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import re\n",
        "from urllib.parse import urljoin\n",
        "from goose3 import Goose\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.dammit import EncodingDetector\n",
        "from google.colab import files\n",
        "\n",
        "#Get the Current Year\n",
        "now = datetime.date.today()\n",
        "year = now.year\n",
        "currentyear = year.__str__()\n",
        "#Get the Curent Month\n",
        "month = (now.month)\n",
        "if(month == 1 or month == 2 or month == 3 or month == 4 or month == 5 or month == 6 or month == 7 or month == 8 or month == 9):\n",
        "    currentmonth = '{:02d}'.format(month)\n",
        "    currentmonth = currentmonth.__str__()\n",
        "else:\n",
        "    currentmonth = month.__str__()\n",
        "\n",
        "#Enter the URL to check for news\n",
        "url = 'https://www.opindia.com/'\n",
        "#Append this url for all the articles\n",
        "url1 = 'https://www.opindia.com/'\n",
        "#Match the pattern in all the url's\n",
        "pattern = 'https://www.opindia.com/'+currentyear+'/'+currentmonth+'/'\n",
        "#Get Response of url\n",
        "response = requests.get(url)\n",
        "http_encoding = response.encoding if 'charset' in response.headers.get('content-type', '').lower() else None\n",
        "html_encoding = EncodingDetector.find_declared_encoding(response.content, is_html=True)\n",
        "encoding = html_encoding or http_encoding\n",
        "soup = BeautifulSoup(response.content, from_encoding=encoding)\n",
        "\n",
        "final_urls = []\n",
        "#Get the list of hyperlink on the page\n",
        "for link in soup.find_all('a'):\n",
        "  href = link.attrs.get(\"href\")\n",
        "  href = urljoin(url1, href)\n",
        "  if pattern in href:\n",
        "    final_urls.append(href)\n",
        "\n",
        "# #Remove duplicate URL's        \n",
        "unique_urls = set(final_urls)\n",
        "# #Convert it back to lists\n",
        "unique_urls_list = list(unique_urls)\n",
        "\n",
        "#Get the Title, Author and Text of Each URL\n",
        "#Create the lists of all variables\n",
        "final_title_list = []\n",
        "final_text_list = []\n",
        "final_source_list = []\n",
        "final_image_list = []\n",
        "for data in unique_urls_list:\n",
        "  g = Goose()\n",
        "  article = g.extract(url=data)\n",
        "  title = article.title\n",
        "  text = article.cleaned_text\n",
        "  domain = article.domain\n",
        "  image = article.infos\n",
        "  print(image)\n",
        "  source=re.findall(r'(?<=\\.)([^.]+)(?:\\.(?:co\\.uk|ac\\.us|[^.]+(?:$|\\n)))',domain)\n",
        "  final_title_list.append(title)\n",
        "  final_text_list.append(text)\n",
        "  final_source_list.extend(source)\n",
        "#   dict_1 = {'title': final_title_list, 'text': final_text_list, 'author':final_source_list}\n",
        "#   df = pd.DataFrame({key: pd.Series(value) for key, value in dict_1.items()})\n",
        "#   df.to_csv('OPTimes_DataSet.csv', encoding='utf-8', index=False)\n",
        "# #Download the file in your local\n",
        "# files.download(\"OPTimes_DataSet.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcOh8mWyREpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "outputId": "df973a33-208a-47a4-cc94-debfa25acee8"
      },
      "source": [
        "#Postcard News Scraping\n",
        "!pip install goose3\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "from urllib.parse import urljoin\n",
        "from goose3 import Goose\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.dammit import EncodingDetector\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "#Enter the URL to check for news\n",
        "url = 'https://postcard.news/'\n",
        "#Append this url for all the articles\n",
        "# url1 = 'https://news.google.com'\n",
        "#Match the pattern in all the url's\n",
        "# pattern = 'https://news.google.com/articles'\n",
        "#Get Response of url\n",
        "response = requests.get(url)\n",
        "http_encoding = response.encoding if 'charset' in response.headers.get('content-type', '').lower() else None\n",
        "html_encoding = EncodingDetector.find_declared_encoding(response.content, is_html=True)\n",
        "encoding = html_encoding or http_encoding\n",
        "soup = BeautifulSoup(response.content, from_encoding=encoding)\n",
        "\n",
        "final_urls = []\n",
        "#Get the list of hyperlink on the page\n",
        "for link in soup.find_all('a'):\n",
        "  href = link.attrs.get(\"href\")\n",
        "  final_urls.append(href)\n",
        "  # print(href)\n",
        "  # href = urljoin(url1, href)\n",
        "#   if pattern in href:\n",
        "#     # final_urls = [href]\n",
        "#     final_urls.append(href)\n",
        "\n",
        "#Remove duplicate URL's        \n",
        "unique_urls = set(final_urls)\n",
        "#Convert it back to lists\n",
        "unique_urls_list = list(unique_urls)\n",
        "\n",
        "#Get the Title, Author and Text of Each URL\n",
        "#Create the lists of all variables\n",
        "final_title_list = []\n",
        "final_text_list = []\n",
        "final_source_list = []\n",
        "for data in unique_urls_list:\n",
        "  request = request.get(data, timeout=none)\n",
        "  g = Goose()\n",
        "  article = g.extract(url=data)\n",
        "  title = article.title\n",
        "  text = article.cleaned_text\n",
        "  domain = article.domain\n",
        "  source=re.findall(r'(?<=\\.)([^.]+)(?:\\.(?:co\\.uk|ac\\.us|[^.]+(?:$|\\n)))',domain)\n",
        "  final_title_list.append(title)\n",
        "  final_text_list.append(text)\n",
        "  final_source_list.extend(source)\n",
        "print(final_title_list)\n",
        "print(final_text_list)\n",
        "print(final_source_list)\n",
        "#   dict_1 = {'title': final_title_list, 'text': final_text_list, 'author':final_source_list}\n",
        "#   df = pd.DataFrame({key: pd.Series(value) for key, value in dict_1.items()})\n",
        "#   df.to_csv('test.csv', encoding='utf-8', index=False)\n",
        "#Download the file in your local\n",
        "# files.download(\"test.csv\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting goose3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/0e/5d049211226268ebce83ae5c8c4f578af0f5f120b24de9542485efcfeda2/goose3-3.1.6-py3-none-any.whl (86kB)\n",
            "\r\u001b[K     |███▉                            | 10kB 16.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 2.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from goose3) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from goose3) (2.8.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from goose3) (4.6.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from goose3) (3.2.5)\n",
            "Collecting cssselect\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (from goose3) (0.42.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from goose3) (7.0.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from goose3) (4.2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->goose3) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->goose3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->goose3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->goose3) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->goose3) (1.12.0)\n",
            "Installing collected packages: cssselect, goose3\n",
            "Successfully installed cssselect-1.1.0 goose3-3.1.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9a175c7e2555>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mfinal_source_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_urls_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'request' is not defined"
          ]
        }
      ]
    }
  ]
}